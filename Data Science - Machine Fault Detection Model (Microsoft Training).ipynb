{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score a machine fault detection model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"17c607cb-082d-40d5-9617-f151b0d65fce"},{"cell_type":"markdown","source":["## Introduction\n","\n","In this notebook, you learn the data science workflow with an end-to-end example. The scenario is to use machine learning to have a more systematic approach to fault diagnosis in order to proactively identify issues and take actions before a machine's actual failure. In this scenario, the aim is to predict whether a machine would experience a failure based on features such as process temperature, rotational speed, etc. \n","\n","The summary of main steps you take in this notebook are:\n","\n","1. Install custom libraries\n","2. Load the data\n","3. Understand and process the data through exploratory data analysis\n","4. Train machine learning models using Scikit-Learn and XGBoost. Track experiments using MLflow and Fabric Autologging feature \n","5. Score the trained model using Fabric PREDICT feature, save the best model, and load it for predictions\n","6. Demonstrate the model performance via visualizations in Power BI\n","\n","Source: Microsoft Training\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a8b81d35-d2b9-4239-8fa0-5c5164829a2e"},{"cell_type":"markdown","source":["## Prerequisites\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b82359c0-2217-4ba5-beb4-ab1aad67cb77"},{"cell_type":"markdown","source":["## Step 1: Install custom libraries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e465f36a-52ef-4df0-9f6f-ffef97822cda"},{"cell_type":"markdown","source":["When developing a machine learning model or doing ad-hoc data analysis, you may need to quickly install a custom library (e.g., `imblearn` in this notebook) for your Apache Spark session. To install a library, you have two choices. \n","\n","1. You can use the in-line installation capabilities (for example, `%pip`, `%conda`, etc.) to quickly get started with new libraries. Note that installing this only installs the custom libraries in the current notebook and not in the workspace.\n","\n","```python\n","# Use pip to install libraries\n","%pip install <library name>\n","\n","# Use conda to install libraries\n","%conda install <library name>\n"," \n","```\n","2. Alternatively, you can follow the instructions [here](https://aka.ms/fabric/create-environment) to learn how to create an environment which allows you to install libraries from public sources or upload custom libraries built by you or your organization."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cd480161-cc96-409a-b624-7366197985a6"},{"cell_type":"markdown","source":["In this notebook, you need to install Imbalanced-learn (imported as `imblearn`) which is a library for synthetic minority oversampling technique (SMOTE). You will proceed with `%pip install` as was discussed in the first approach. Note that the PySpark kernel will be restarted after `%pip install`, thus you need to install the library before you run any other cells."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a8b39c29-58f7-4f70-8ac3-d9f6e617f5b0"},{"cell_type":"code","source":["# Using pip to install imblearn\n","%pip install imblearn"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[3,4,5,6,7,8],"state":"finished","livy_statement_state":"available","session_id":"8de4cf6f-a225-4178-aef5-eff9864adf3a","normalized_state":"finished","queued_time":"2024-10-06T04:41:15.08578Z","session_start_time":"2024-10-06T04:41:15.3686422Z","execution_start_time":"2024-10-06T04:41:24.074895Z","execution_finish_time":"2024-10-06T04:41:43.168394Z","parent_msg_id":"1b224925-a38d-4cb3-a18a-d57b0c50747e"},"text/plain":"StatementMeta(, 8de4cf6f-a225-4178-aef5-eff9864adf3a, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting imblearn\n  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\nCollecting imbalanced-learn (from imblearn)\n  Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.24.3)\nRequirement already satisfied: scipy>=1.5.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.10.1)\nRequirement already satisfied: scikit-learn>=1.0.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.2.0)\nInstalling collected packages: imbalanced-learn, imblearn\nSuccessfully installed imbalanced-learn-0.12.4 imblearn-0.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a1f8636e-5196-4e36-ace7-59ca190e3f80"},{"cell_type":"markdown","source":["## Step 2: Load the data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7f8f9c6e-b18d-4aca-97f5-a3421b631244"},{"cell_type":"markdown","source":["### Dataset\n\nThe dataset simulates logging of a manufacturing machine's parameters as a function of time, which is common in industrial settings. It consists of 10,000 data points stored as rows with features as columns. The features include:\n1. A Unique Identifier (UID) ranging from 1 to 10000. \n2. Product ID, consisting of a letter L, M, or H, indicating the product quality variant, and a variant-specific serial number. Low, medium, and high-quality variants make up 60%, 30%, and 10% of all products, respectively.\n3. Air temperature in Kelvin.\n4. Process Temperature in Kelvin. \n5. Rotational Speed in rpm.\n6. Torque in Nm.\n7. Tool Wear in minutes. The quality variants H, M, and L add 5, 3, and 2 minutes of tool wear to the used tool in the process, respectively.\n8. Machine Failure Label, indicating whether the machine has failed in this particular data point for any of the following five independent failure modes:\n<br/><br/>\n    - Tool Wear Failure (TWF): the tool is replaced or fails at a randomly selected tool wear time between 200 and 240 minutes.\n    - Heat Dissipation Failure (HDF): heat dissipation causes a process failure if the difference between air and process temperature is below 8.6 K and the tool's rotational speed is below 1380 rpm.\n    - Power Failure (PWF): the product of torque and rotational speed (in rad/s) equals the power required for the process. The process fails if this power is below 3500 W or above 9000 W.\n    - OverStrain Failure (OSF): if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain.\n    - Random Failures (RNF): each process has a chance of 0.1% to fail regardless of its process parameters.\n\n> [!NOTE]\n> If at least one of the above failure modes is true, the process fails, and the `machine failure` label is set to 1. It's therefore not transparent to the machine learning method, which of the failure modes has caused the process to fail."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fe215529-c7f6-4864-b2a1-36a85e676ac5"},{"cell_type":"markdown","source":["### Download dataset and upload to lakehouse\n","\n","Connect to Azure Open Datasets Container and load the Predictive Maintenance dataset. This code downloads a publicly available version of the dataset and then stores it in a Fabric lakehouse.\n","\n","> [!IMPORTANT]\n","> [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. **Failure to do so results in an error.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4ee4092a-a8e1-435b-9ffd-92d77bfed0a6"},{"cell_type":"code","source":["# Download demo data files into lakehouse if not exist\n","import os, requests\n","DATA_FOLDER = \"Files/predictive_maintenance/\"  # Folder containing the dataset\n","DATA_FILE = \"predictive_maintenance.csv\"  # Data file name\n","remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/MachineFaultDetection\"\n","file_list = [\"predictive_maintenance.csv\"]\n","download_path = f\"/lakehouse/default/{DATA_FOLDER}/raw\"\n","\n","if not os.path.exists(\"/lakehouse/default\"):\n","    raise FileNotFoundError(\n","        \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","    )\n","os.makedirs(download_path, exist_ok=True)\n","for fname in file_list:\n","    if not os.path.exists(f\"{download_path}/{fname}\"):\n","        r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n","        with open(f\"{download_path}/{fname}\", \"wb\") as f:\n","            f.write(r.content)\n","print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"233a7b5f-19c2-4575-879c-6538069fea77"},{"cell_type":"markdown","source":["Start recording the time it takes to run this notebook."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"64025cfc-77ce-474b-ac13-0a5a55d5f97f"},{"cell_type":"code","source":["# Record the notebook running time\n","import time\n","\n","ts = time.time()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"affe159f-01a7-476f-9782-3e0d617c0e9a"},{"cell_type":"markdown","source":["Once the dataset is downloaded into the lakehouse, you can load it as a spark DataFrame. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"219750b3-d071-44f8-ac93-17492bc80c11"},{"cell_type":"code","source":["df = (\n","    spark.read.option(\"header\", True)\n","    .option(\"inferSchema\", True)\n","    .csv(f\"{DATA_FOLDER}raw/{DATA_FILE}\")\n","    .cache()\n",")\n","df.show(5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1d4d4445-554d-44f0-ac52-3968e8fc647e"},{"cell_type":"markdown","source":["### Write Spark DataFrame to lakehouse delta table\n","\n","Format the data (for example, replace space with underscore) to facilitate spark operations in subsequent steps. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"40665025-a328-47c7-a30e-e8f8b51069b5"},{"cell_type":"code","source":["# Replace space in column name with underscore to avoid invalid character while saving \n","df = df.toDF(*(c.replace(' ', '_') for c in df.columns))\n","table_name = \"predictive_maintenance_data\"\n","df.show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c28272ae-87db-4228-a386-43f79ff3f86f"},{"cell_type":"code","source":["# Save data with processed columns to the lakehouse \n","df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dba7e74b-6983-47bc-a89f-50ecdec1cfe3"},{"cell_type":"markdown","source":["## Step 3: Preprocess data and perform exploratory data analysis "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2f25d20a-f40c-4095-9027-d9e3c257d95c"},{"cell_type":"markdown","source":["Convert spark DataFrame to pandas dataFrame to use pandas compatible popular plotting libraries. Note that for large datasets, you may need to load a portion of the dataset. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ed8de02c-7160-4051-97e6-f644831bb0e2"},{"cell_type":"code","source":["data = spark.read.format(\"delta\").load(\"Tables/predictive_maintenance_data\")\n","SEED = 1234\n","df = data.toPandas()\n","df.drop(['UDI', 'Product_ID'],axis=1,inplace=True)\n","# Rename the Target column to IsFail\n","df = df.rename(columns = {'Target': \"IsFail\"})\n","df.info()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"555edcb2-dfb9-4464-9432-3d04a508b8dc"},{"cell_type":"markdown","source":["Convert specific columns of the dataset to floats and integer types and map strings such as [`L`, `M`, `H`] to numerical values [0, 1, 2]."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4ae92a93-dd5c-429c-8f36-a3318fb960d5"},{"cell_type":"code","source":["# Convert temperature, rotational speed, torque, and tool wear columns to float\n","df['Air_temperature_[K]'] = df['Air_temperature_[K]'].astype(float)\n","df['Process_temperature_[K]'] = df['Process_temperature_[K]'].astype(float)\n","df['Rotational_speed_[rpm]'] = df['Rotational_speed_[rpm]'].astype(float)\n","df['Torque_[Nm]'] = df['Torque_[Nm]'].astype(float)\n","df['Tool_wear_[min]'] = df['Tool_wear_[min]'].astype(float)\n","\n","# Convert the 'Target' column to integer \n","df['IsFail'] = df['IsFail'].astype(int)\n","# Map 'L', 'M', 'H' to numerical values \n","df['Type'] = df['Type'].map({'L': 0, 'M': 1, 'H': 2})"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8b6b8d1-e340-4d8b-81ca-933e26bbbf6a"},{"cell_type":"markdown","source":["### Explore data through visualizations"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c42965c0-1aa6-41e9-a0e3-cab18a5ec2f4"},{"cell_type":"code","source":["# Import packages and set plotting style\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","sns.set_style('darkgrid')\n","\n","# Create the correlation matrix\n","corr_matrix = df.corr(numeric_only=True)\n","\n","# Plot a heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True)\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c4a4d96-86ae-4325-abe9-ae3440e982c3"},{"cell_type":"markdown","source":["The correlation matrix shows that `Air_temperature`, `Process_temperature`, `Rotational_speed`, `Torque`, and `Tool_wear` have the highest correlation with the `IsFail` variable."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1e80d69c-6e53-4c24-9feb-3895684e1e4b"},{"cell_type":"code","source":["# Plot histograms of select features\n","fig, axes = plt.subplots(2, 3, figsize=(18,10))\n","columns = ['Air_temperature_[K]', 'Process_temperature_[K]', 'Rotational_speed_[rpm]', 'Torque_[Nm]', 'Tool_wear_[min]']\n","data=df.copy()\n","for ind, item in enumerate (columns):\n","    column = columns[ind]\n","    df_column = data[column]\n","    df_column.hist(ax = axes[ind%2][ind//2], bins=32).set_title(item)\n","fig.supylabel('count')\n","fig.subplots_adjust(hspace=0.2)\n","fig.delaxes(axes[1,2])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b9acf2a-4510-4f76-b2b2-583601754c7d"},{"cell_type":"markdown","source":["As can be seen from the plotted graphs, the `Air_temperature`, `Process_temperature`, `Rotational_speed`, `Torque`, and `Tool_wear` variables aren't sparse and appear to have good continuity in the feature space. These plots confirm that training a machine learning model on this dataset is likely to produce results that are reliable and can be generalized to new dataset.    "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f43bb8a2-a7ba-466d-9b0c-1ed93ea9d649"},{"cell_type":"markdown","source":["### Inspect the target variable for class imbalance \n","\n","Count the number of samples for failed and unfailed machines and inspect the data balance for each class (`IsFail`=0, `IsFail`=1). "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3347ff04-7294-45c7-a2a3-9c741f07a453"},{"cell_type":"code","source":["# Plot the counts for no-failure and each failure types\n","plt.figure(figsize=(12, 2))\n","ax = sns.countplot(x='Failure_Type', data=df)\n","for p in ax.patches:\n","    ax.annotate(f'{p.get_height()}', (p.get_x()+0.4, p.get_height()+50))\n","\n","plt.show()\n","\n","# Plot the counts for no failure versus sum of all failure types\n","plt.figure(figsize=(4, 2))\n","ax = sns.countplot(x='IsFail', data=df)\n","for p in ax.patches:\n","    ax.annotate(f'{p.get_height()}', (p.get_x()+0.4, p.get_height()+50))\n","\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f7b69c3-3203-4612-a7f8-a67a0c44ada9"},{"cell_type":"markdown","source":["The plots indicate that the no failure class (shown as `IsFail=0` in the second plot) constitutes most of the samples. Use an oversampling technique to create a more balanced training dataset."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"aca13dde-1bec-4092-b0b5-92df24311bca"},{"cell_type":"code","source":["# Separate features and target\n","features = df[['Type', 'Air_temperature_[K]', 'Process_temperature_[K]', 'Rotational_speed_[rpm]', 'Torque_[Nm]', 'Tool_wear_[min]']]\n","labels = df['IsFail']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29daffc3-cea3-43ac-b348-c45d2d18645b"},{"cell_type":"code","source":["# Split the dataset into the train and test sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa7c7990-5a4e-421f-aed9-29f69071b79b"},{"cell_type":"code","source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Save test data to lakehouse for use in future\n","table_name = \"predictive_maintenance_test_data\"\n","df_test_X = spark.createDataFrame(X_test)\n","df_test_X.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8b539a1-20cd-4e2f-9fc6-a88c94ca7cee"},{"cell_type":"markdown","source":["### Oversample to balance classes the training dataset\n","\n","The previous analysis showed that the dataset is highly imbalanced. The problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. \n","\n","Use [<span style=\"color:blue\">SMOTE</span>](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html), which is a widely used oversampling technique that generates synthetic examples. It generates examples for the minority class based on the distances between data points using Euclidean distance. This method is different from random oversampling as it creates new examples that aren't just duplicates of the minority class, making it a more effective technique for imbalanced datasets.\n","\n","Note that you will be able to access SMOTE using the `imblearn` library that you installed in Step 1.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e688e1fc-2d8b-4764-afd2-57d6334f11dc"},{"cell_type":"code","source":["# Disable MLflow autologging to avoid tracking the SMOTE fitting\n","import mlflow\n","\n","mlflow.autolog(disable=True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"482a4ba9-c71c-454b-8b31-14121afe2dac"},{"cell_type":"code","source":["from imblearn.combine import SMOTETomek\n","smt = SMOTETomek(random_state=SEED)\n","X_train_res, y_train_res = smt.fit_resample(X_train, y_train)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ab8ca69-60bc-469d-82f5-14994cd9e40c"},{"cell_type":"code","source":["# Plot the counts for both classes\n","plt.figure(figsize=(4, 2))\n","ax = sns.countplot(x='IsFail', data=pd.DataFrame({'IsFail': y_train_res.values}))\n","for p in ax.patches:\n","    ax.annotate(f'{p.get_height()}', (p.get_x()+0.4, p.get_height()+50))\n","\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2823846-fa76-4871-877c-7545d7110382"},{"cell_type":"markdown","source":["You have successfully balanced the dataset and can move to model training."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"05640e2f-92f0-41c6-bc11-ddaf5ad81231"},{"cell_type":"markdown","source":["## Step 4: Train and evaluate the model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dafe8a6a-ebb5-4836-ba23-e9958973326c"},{"cell_type":"markdown","source":["[MLflow](https://aka.ms/fabric-autologging) is used to register models, train and compare various models, and pick the best model for prediction purpose. For model training, you use the following 3 models:\n","\n","1. Random forest classifier\n","2. Logistic regression classifier \n","3. XGBoost classifier "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c6d256c9-a45b-4dd5-a51b-7dc001586aa5"},{"cell_type":"markdown","source":["### Train a Random Forest classifier"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dbc33eb1-68de-4971-9782-57f36728a3fb"},{"cell_type":"code","source":["import numpy as np \n","from sklearn.ensemble import RandomForestClassifier\n","from mlflow.models.signature import infer_signature\n","from sklearn.metrics import f1_score, accuracy_score, recall_score\n","\n","mlflow.set_experiment(\"Machine_Failure_Classification\")\n","mlflow.autolog(exclusive=False) # Override the pre-configured autologging behavior\n","\n","with mlflow.start_run() as run:\n","    rfc_id = run.info.run_id\n","    print(f\"run_id {rfc_id}, status: {run.info.status}\")\n","    rfc = RandomForestClassifier(max_depth=5, n_estimators=50)\n","    rfc.fit(X_train_res, y_train_res) \n","    signature = infer_signature(X_train_res, y_train_res)\n","\n","    mlflow.sklearn.log_model(\n","        rfc,\n","        \"machine_failure_model_rf\",\n","        signature=signature,\n","        registered_model_name=\"machine_failure_model_rf\"\n","    ) \n","\n","    y_pred_train = rfc.predict(X_train)\n","    # Calculate the classification metrics for test data\n","    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n","    accuracy_train = accuracy_score(y_train, y_pred_train)\n","    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_train\", f1_train)\n","    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n","    mlflow.log_metric(\"recall_train\", recall_train)\n","\n","    # Print the run ID and the classification metrics\n","    print(\"F1 score_train:\", f1_train)\n","    print(\"Accuracy_train:\", accuracy_train)\n","    print(\"Recall_train:\", recall_train)    \n","\n","    y_pred_test = rfc.predict(X_test)\n","    # Calculate the classification metrics for test data\n","    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n","    accuracy_test = accuracy_score(y_test, y_pred_test)\n","    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_test\", f1_test)\n","    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n","    mlflow.log_metric(\"recall_test\", recall_test)\n","\n","    # Print the classification metrics\n","    print(\"F1 score_test:\", f1_test)\n","    print(\"Accuracy_test:\", accuracy_test)\n","    print(\"Recall_test:\", recall_test)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd1e6471-53ef-4f7e-89b4-f114205a2a5c"},{"cell_type":"markdown","source":["As you can see, both train and test dataset yield F1 score, accuracy and recall of approximately 0.9 using Random Forest classifier. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9e451d82-385b-4ad5-bac7-393c79b83d2c"},{"cell_type":"markdown","source":["### Train a Logistic Regression classifier"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bc4c81a2-54b5-4475-89a7-1fb367271227"},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","with mlflow.start_run() as run:\n","    lr_id = run.info.run_id\n","    print(f\"run_id {lr_id}, status: {run.info.status}\")\n","    lr = LogisticRegression(random_state=42)\n","    lr.fit(X_train_res, y_train_res)\n","    signature = infer_signature(X_train_res, y_train_res)\n","  \n","    mlflow.sklearn.log_model(\n","        lr,\n","        \"machine_failure_model_lr\",\n","        signature=signature,\n","        registered_model_name=\"machine_failure_model_lr\"\n","    ) \n","\n","    y_pred_train = lr.predict(X_train)\n","    # Calculate the classification metrics for train data\n","    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n","    accuracy_train = accuracy_score(y_train, y_pred_train)\n","    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_train\", f1_train)\n","    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n","    mlflow.log_metric(\"recall_train\", recall_train)\n","\n","    # Print the run ID and the classification metrics\n","    print(\"F1 score_train:\", f1_train)\n","    print(\"Accuracy_train:\", accuracy_train)\n","    print(\"Recall_train:\", recall_train)    \n","\n","    y_pred_test = lr.predict(X_test)\n","    # Calculate the classification metrics for test data\n","    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n","    accuracy_test = accuracy_score(y_test, y_pred_test)\n","    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_test\", f1_test)\n","    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n","    mlflow.log_metric(\"recall_test\", recall_test)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5da19d14-d20d-4289-89f2-de5a954c621c"},{"cell_type":"markdown","source":["### Train an XGBoost classifier"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0f26a8de-86b7-47fb-9d14-b7a439ae9997"},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","\n","with mlflow.start_run() as run:\n","    xgb = XGBClassifier()\n","    xgb_id = run.info.run_id \n","    print(f\"run_id {xgb_id}, status: {run.info.status}\")\n","    xgb.fit(X_train_res.to_numpy(), y_train_res.to_numpy()) \n","    signature = infer_signature(X_train_res, y_train_res)\n","  \n","    mlflow.xgboost.log_model(\n","        xgb,\n","        \"machine_failure_model_xgb\",\n","        signature=signature,\n","        registered_model_name=\"machine_failure_model_xgb\"\n","    ) \n","\n","    y_pred_train = xgb.predict(X_train)\n","    # Calculate the classification metrics for train data\n","    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n","    accuracy_train = accuracy_score(y_train, y_pred_train)\n","    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_train\", f1_train)\n","    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n","    mlflow.log_metric(\"recall_train\", recall_train)\n","\n","    # Print the run ID and the classification metrics\n","    print(\"F1 score_train:\", f1_train)\n","    print(\"Accuracy_train:\", accuracy_train)\n","    print(\"Recall_train:\", recall_train)    \n","\n","    y_pred_test = xgb.predict(X_test)\n","    # Calculate the classification metrics for test data\n","    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n","    accuracy_test = accuracy_score(y_test, y_pred_test)\n","    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n","\n","    # Log the classification metrics to MLflow\n","    mlflow.log_metric(\"f1_score_test\", f1_test)\n","    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n","    mlflow.log_metric(\"recall_test\", recall_test)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7f6b8b0-2a92-4430-8eeb-a25850d124c2"},{"cell_type":"markdown","source":["## Step 5: Select model and predict outputs"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fdec1ca7-a8dd-42cd-9fe2-af36f1700f90"},{"cell_type":"markdown","source":["In the previous section, you trained three different classifiers: Random Forest, Logistic Regression, and XGBoost. You have the choice to either programatically access the results or use the user interface (UI).\n","\n","To use the UI path, navigate to your workspace and filter the models. Then select individual models for details of the model performance. \n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bce86f42-5abd-4d24-b95f-1075a07267db"},{"cell_type":"markdown","source":["<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/MachineFaultDetection/Model_inspection.png\"  width=\"60%\" height=\"10%\"> \n","\n","\n","\n","<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/MachineFaultDetection/Model_metrics.png\"  width=\"60%\" height=\"10%\">\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a2b292c1-bee4-4b53-833b-ccf89ac48d4d"},{"cell_type":"markdown","source":["Alternatively, use the following example to learn how to programatically access the models through MLflow."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"42f37ac4-59cf-4e1c-bd96-9dad96a84575"},{"cell_type":"code","source":["runs = {'random forest classifer':   rfc_id,\n","        'logistic regression classifier': lr_id,\n","        'xgboost classifier': xgb_id}\n","\n","# Create an empty list to hold the metrics\n","df_metrics = []\n","\n","# Loop through the run IDs and retrieve the metrics for each run\n","for run_name, run_id in runs.items():\n","    metrics = mlflow.get_run(run_id).data.metrics\n","    metrics[\"run_name\"] = run_name\n","    df_metrics.append(metrics)\n","\n","# Convert the list to DataFrame\n","df_metrics = pd.DataFrame(df_metrics)\n","\n","# Print the DataFrame\n","print(df_metrics)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a69c43e2-5ea5-42e3-97b4-e6296aeb9201"},{"cell_type":"markdown","source":["Although XGBoost yielded the best results on the training set, it does perform poorly on the test data set, which indicates overfitting. Logistic Regression classifier performs poorly on both training and test datasets. Overall, Random Forest strikes a good balance between training performance and avoiding overfitting.\n","In the following section, choose the registered Random Forest model and perform prediction using the [PREDICT](https://aka.ms/fabric-predict) feature."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d7a31bd2-66f3-44a1-afc7-f14e8ac42b72"},{"cell_type":"code","source":["from synapse.ml.predict import MLFlowTransformer\n","\n","model = MLFlowTransformer(\n","    inputCols=list(X_test.columns),\n","    outputCol='predictions',\n","    modelName='machine_failure_model_rf',\n","    modelVersion=1\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4b6e0384-e9de-4022-a141-d9c6cc950494"},{"cell_type":"markdown","source":["Now that you've created an MLFLowTransformer object to load the model for inferencing, use the Transformer API to score the model on the test dataset."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f9e7959f-4e7c-4770-87df-c53370288195"},{"cell_type":"code","source":["predictions = model.transform(spark.createDataFrame(X_test))\n","predictions.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f217591-65ab-40c1-89ef-14c381cdd072"},{"cell_type":"markdown","source":["Save the data into the lakehouse which, allows you to access the data for future use such as creating a Power BI dashboard."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d9e8c73-3ee4-4237-83e5-39810f29c0e5"},{"cell_type":"code","source":["# Save test data to lakehouse for use in future\n","table_name = \"predictive_maintenance_test_with_predictions\"\n","predictions.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f79989d-a640-444c-b841-a33cbcfdbe38"},{"cell_type":"markdown","source":["## Step 6: Business Intelligence via Visualizations in Power BI\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"848cfd4d-02a4-4128-8b61-c8f26b4617c3"},{"cell_type":"markdown","source":["You can demonstrate the results in an offline format using a Power BI dashboard. \n","\n","<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/MachineFaultDetection/PowerBI_report_machine_maintenance.png\"  width=\"75%\" height=\"10%\"> "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0911fe9c-1cc2-4a39-92ae-51607a67b46f"},{"cell_type":"markdown","source":["The dashboard shows that the `Tool_wear` and `Torque` create a noticeable boundary between failed and unfailed cases as was expected from the earlier correlation analysis in Step 2."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2575776a-323b-4dac-bb3f-0cec801f3973"},{"cell_type":"code","source":["# Determine the entire runtime\n","print(f\"Full run cost {int(time.time() - ts)} seconds.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcd2ec62-99dc-447f-a32d-904c23b49516"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"dependencies":{"lakehouse":{"default_lakehouse":"696dbd33-0263-4afa-8a4d-da798f2a0107","default_lakehouse_name":"TRAINING_FABRIC_2024","default_lakehouse_workspace_id":"764a0988-7fed-4c47-88ed-dc9201abfc05"}}},"nbformat":4,"nbformat_minor":5}